2025-07-01 07:25:04,708 - INFO - [Rank 0] Initialized wandb at local_rank 0
Successfully loaded data as tensors
Loading dataset
Loading Distributed Sampler for ddp
Training: 100%|##########| 4070/4070 [00:05<00:00, 753.77it/s]
Traceback (most recent call last):
  File "/workspace/HLLM/main/train.py", line 29, in <module>
    trainer.train()
  File "/workspace/HLLM/main/train_utils.py", line 234, in train
    loss.backward()
  File "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 346, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 199, in _make_grads
    raise RuntimeError(
RuntimeError: grad can be implicitly created only for scalar outputs
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/HLLM/main/train.py", line 29, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/workspace/HLLM/main/train_utils.py", line 234, in train
[rank0]:     loss.backward()
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py", line 648, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 346, in backward
[rank0]:     grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 199, in _make_grads
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: grad can be implicitly created only for scalar outputs
