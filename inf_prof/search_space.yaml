mixed_precision: [True, False]
mixed_precision_dtype: ["fp16", "bfloat16", "fp8", "bfloat8", "int8", "int4"]
quantization: [True, False]
quantization_dtype: ["float16", "float8", "bfloat16", "bfloat8", "int8", "int4"]
quantization_method: ["gptq", "awq", "gguf", "exl12", "bitsandbytes", "kv_cache"]
model_execution_backend: ["torch.compile", "onnxruntime", "tensorrt", "vllm"]
torch_compile_backend: ["inductor", "eager", "aot_eager"]
torch_compile_mode: ["default", "reduce-overhead", "max-autotune"]
decoding: ["speculative",  "speculative_top_p",  "speculative_top_k", "base_top_p",  "base_top_k"]