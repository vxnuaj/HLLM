mixed_precision_dtype: ["fp16", "bfloat16", "fp8", "bfloat8", "int8", "int4", None]
quantization_dtype: [3, 4, 8]
quantization_method: ["gptq", "awq", "gguf", "exl12", "bitsandbytes"]
quantize_kvcache: ["float16", "bfloat16", "float8", "bfloat8", "int8", "int4", None]
model_execution_backend: ["torch.compile", "onnxruntime", "tensorrt", "vllm"]
torch_compile_backend: ["inductor", "eager", "aot_eager"]
torch_compile_mode: ["default", "reduce-overhead", "max-autotune"]
decoding: ["speculative",  "speculative_top_p",  "speculative_top_k", "base_top_p",  "base_top_k"]