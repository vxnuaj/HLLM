mixed_precision_dtype: ["fp16", "bfloat16", "fp8", "bfloat8", "int8", "int4", null]
quantization_wbits: [3, 4, 8]
quantization_method: ["gptq", "awq", "quanto", "aqlm", "vptq", "hqq", "bitsandbytes", "spqr", null]
quantize_kvcache: ["float16", "bfloat16", "float8", "bfloat8", "int8", "int4", null]
model_execution_backend: ["torch.compile"]
torch_compile_backend: ["inductor", "eager", "aot_eager"]
torch_compile_mode: ["default", "reduce-overhead", "max-autotune"]
decoding: ["speculative",  "speculative_top_p",  "speculative_top_k", "base_top_p",  "base_top_k"]