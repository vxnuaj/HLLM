quantization_wbits: [3, 4, 8]
quantization_method: ["gptq", "awq", "quanto", "aqlm", "vptq", "hqq", "bitsandbytes", "spqr", null]
quantize_kvcache_method: ["hqq", "quanto", null]
quantize_kvcache: [2, 4, 8, null]
model_execution_backend: ["torch.compile"]
torch_compile_backend: ["inductor", "eager", "aot_eager"]
torch_compile_mode: ["default", "reduce-overhead", "max-autotune"]
decoding: ["speculative", "base"]