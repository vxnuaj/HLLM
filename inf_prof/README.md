### Hyperparameter Sweep over Inference Pass.

- Mixed Precision Inference ( not during quantization)
    - fp16
    - bfloat16
    - fp8
    - bfloat8
- Precision during Quantization
	- float32
	- float16
	- float8
	- bfloat16
	- bfloat8
	- int8
	- int4
- Quantization method(s)
	- GPTQ
	- AWQ
	- GGUF
	- EXL12
	- BitsAndBytes
	- Quantize the KV Kache
- Model Execution Backend
	- torch.compile
		- inductor
		- eager 
		- aot_eager
	- onnx runtime
	- tensorrt
	- vllm
- Decoding
	- Speculative Decoding ( distill the model into a model thatâ€™s 4x smaller ).