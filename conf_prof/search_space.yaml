context_len: [512]
d_model: [512]
n_heads: [8]
n_blocks: [4]
vocab_size: [10000]
pos_emb_dropout_p: [.1]
learned: [False]
ntk_rope_scaling: [False]
dyn_scaling: [False]
attn_type: [mhsa, mqa, gqa, mva]
n_groups: [2, 4]
top_k_sparsev: [null]
p_threshold: [null]
p_threshold_steps_fraction: [null]
flash_attn: [False, True]
pos_emb_type: [rope]
mixed_precision: [True, False]
flash_attn_dtype: ["torch.float16"]
compile: [True]
parallel: ['fsdp', 'ddp']
fsdp_wrap_policy: ['auto', 'transformer']